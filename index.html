<!DOCTYPE html>
<html lang="en">
<!--
   test_tfjs.html
   
   Copyright 2019 yves <yves.mercadier@ac-montpellier.fr>
   
   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; either version 2 of the License, or
   (at your option) any later version.
   
   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.
   
   You should have received a copy of the GNU General Public License
   along with this program; if not, write to the Free Software
   Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
   MA 02110-1301, USA.
   
   
-->

<head>
<meta charset="utf-8" />
<title>NLP and Text classification with tensorflowjs</title>
<meta name="generator" content="Geany 1.33" />
<link rel="stylesheet" type="text/css" href="./styles.css" />

<!-- Google Fonts, Normalize, and Font Awesome -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Cardo|Montserrat:300,400,500&amp;subset=latin-ext" crossorigin="anonymous" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" integrity="sha256-oSrCnRYXvHG31SBifqP2PM1uje7SJUyX0nTwO2RJV54=" crossorigin="anonymous" />
<link rel="stylesheet" type="text/css" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css" integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt" crossorigin="anonymous" />
</head>

<body>

<header>
            <h1>You think you have humor ! That seems a little presumptuous to me. Let's check</h1>
</header>
	
<section>
	
Artificial intelligence algorithms have no humor, however are they capable of detecting yours? This is what we will see in this small demonstration.
<br>
Text classification consists of associating a label with each text.Tensorflowjs is a fantastic tool for performing neural network text classification and this can be done inside a web page.
<br>
In this example, we are going to train a neural network to recognize if a joke is funny or if it is not funny.
<br>
Click on the button to start the learning process of the neural network integrated into this web page :<div id="action"></div>
	
<input class="styled" type="button" value="START TRAINNING" id="train">
<input class="styled" type="button" value="VIEW DATASET" id="data">

</section>

	
<footer>
	footer
        <!--mettre ici l adresse mail et le telephone-->   
 </footer>


<!-- TensorFlow.js is loading-->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.0.2/dist/tfjs-vis.umd.min.js"></script>

</body>

<script>

//var btn = document.querySelector('input');
var btntrain = document.getElementById('train');
var btndata = document.getElementById('data');
btntrain.addEventListener('click', deeplearning);
btndata.addEventListener('click', viewdataset);
	
//var dataset=[["new york was originally dutch", ["new york"]], ["the big apple is great", ["new york"]], ["new york is also called the big apple", ["new york"]], ["nyc is nice", ["new york"]], ["people abbreviate new york city as nyc", ["new york"]], ["the capital of great britain is london", ["london"]], ["london is in the uk", ["london"]], ["london is in england", ["london"]], ["london is in great britain", ["london"]], ["it rains a lot in london", ["london"]], ["london hosts the british museum", ["london"]], ["new york is great and so is london", ["new york", "london"]], ["i like london better than new york", ["new york", "london"]], ["nice day in nyc", ["new york"]], ["welcome to london", ["london"]], ["london is rainy", ["london"]], ["it is raining in britian", ["london"]], ["it is raining in britian and the big apple", ["new york", "london"]], ["it is raining in britian and nyc", ["new york", "london"]], ["hello welcome to new york. enjoy it here and london too", ["new york", "london"]]]
var dataset=[["What do you call a potato in space? Spudnik", ["funny"]], ["My son just got a tattoo of a heart, a spade, a club, and a diamond, all without my permission. I guess I'll deal with him later.", ["funny"]], ["What did Arnold Schwarzenegger say at the abortion clinic? Hasta last vista, baby.", ["funny"]], ["How to get a cop's attention", ["not funny"]], ["Why did the chicken hold a seance? To get to the other side.", ["funny"]], ["Mom: Do you want this? Me: No. Mom: Ok I'll give it to your brother. Me: No I want it.", ["funny"]], ["How do you fit 4 gays on one barstool? Flip it over!", ["not funny"]], ["Before I destroy a wasp's nest I like to capture a single wasp and tell it my entire diabolical plan.", ["not funny"]], ["INTERVIEWER: Why do you want to work here? ME: *crumbs tumbling from my mouth* Oh, I don't. I was just walking by and saw you had donuts.", ["funny"]], ["What's the difference between a blonde and a washer? When you dump your load in a washer, it doesn't follow you around for a week.", ["funny"]], ["I've struggled for years to be above the influence... But I've never been able to get that high", ["not funny"]], ["Bill Clinton must be the luckiest man in the world. All of the sex he has, with Hillary, you know it's hate sex.", ["not funny"]], ["I don't believe in Bigfoot; because he never believed in me. I'd scan the crowd at my ballet recitals, and always see that one empty seat.", ["funny"]], ["I often think if I'd taken a different path in life, I could be lying on a slightly more comfortable sofa right now.", ["funny"]], ["You know what, we need a huge spoon to take care of this -Guy who invented shovels", ["funny"]],["What do people from the 1930's and /r/news jokes have in common? They're both old.", ["funny"]], ["What do you call a blind dinosaur? A do-think-he-saurus :) !! Lol What do you call a blind dinosaurs dog? A do-you-think-he-saurus-rex", ["funny"]], ["How do you know if your wine was made in the 90's? It smells like teen spirit.", ["funny"]]]

function viewdataset() {
	function generate_table() {
  	// get the reference for the body
  	var body = document.getElementsByTagName("body")[0];
  	var section = document.getElementsByTagName("section")[0];

  	// creates a <table> element and a <tbody> element
  	var tbl = document.createElement("table");
	tbl.id = "tabledata";
  	var tblBody = document.createElement("tbody");

  	// creating all cells
  	for (var i = 0; i < dataset.length; i++) {
  	  // creates a table row
   	 var row = document.createElement("tr");

   	 var cell1 = document.createElement("td");
   	 var cellText1 = document.createTextNode(dataset[i][0]);
   	 cell1.appendChild(cellText1);
   	 row.appendChild(cell1);
	var cell2 = document.createElement("td");
   	 var cellText2 = document.createTextNode(dataset[i][1]);
   	 cell2.appendChild(cellText2);
   	 row.appendChild(cell2);
   	 // add the row to the end of the table body
  	 tblBody.appendChild(row);
  }

  // put the <tbody> in the <table>
  tbl.appendChild(tblBody);
  // appends <table> into <body>
  section.appendChild(tbl);
  // sets the border attribute of tbl to 2;
  tbl.setAttribute("border", "2");
}
	var el=document.getElementById('tabledata')
	if(!el){generate_table();}
	else{el.parentNode.removeChild(el);}
}
	
function deeplearning() {
document.getElementById("action" ).innerHTML = 'in action';

const  sequenceLength=7
const epochs=15
const proportion_train=0.8

var doc_train= [];
var label_train= [];
for (var i in dataset) {
	doc_train.push(dataset[i][0])
	label_train.push(dataset[i][1][0])
	}
var dico_word=[]
var x=[]
var max_len=0
for (let doc of doc_train) {
	//creation of the id of each word in a dictionary.
	var words = doc.split(' ');
	for (let w of words){
		if (dico_word.indexOf(w)==-1){dico_word.push(w)}
		}
	//digitization of documents.
	var sequence=[]
	for (let w of words){
		var id=dico_word.indexOf(w)
		sequence.push(id)
		}
	x.push(sequence)
	//pad
	if(max_len<words.length){max_len=words.length}
	}
//pad sequence
for (var i in x) {
	//we add the zeros in the end if it is necessary.
	while (x[i].length<max_len){x[i].push(0)}
	//we cut the sequence to the desired size
	x[i]=x[i].slice(0,sequenceLength)
}

//define embedding shape matrix

const inputDim    = dico_word.length;
const outputDim = 150;

//digitization of labels.

dico_label=[]
for(let label of label_train){
	if (dico_label.indexOf(label)==-1){dico_label.push(label)}
	}
var y=[]
const nb_class=dico_label.length
for(let label of label_train){
	let vector= []
	for(let  i in dico_label){
		if(dico_label[i].indexOf(label)){vector[i]=1}
		else{vector[i]=0}
		}
	y.push(vector)
}

//definition of the set of train and test

var limite=Math.round(x.length*proportion_train)
x_train = tf.tensor2d(x.slice(0, limite));
y_train = tf.tensor2d(y.slice(0, limite));
x_test= tf.tensor2d(x.slice(limite, x.length));
y_test = y.slice(limite, y.length);

// Define a model
const model = tf.sequential();

//Add layers to model
model.add(tf.layers.embedding({
inputDim: inputDim,
outputDim: outputDim,
inputLength: sequenceLength,
trainable: true
}));
// tf.layers.embedding({inputDim: embeddingMatrix.shape[0],outputDim: embeddingMatrix.shape[1],trainable: false,weights: [embeddingMatrix]})
model.add(
	tf.layers.bidirectional({layer: tf.layers.lstm({units: 256, returnSequences: true})})
	);  

model.add(tf.layers.flatten());
model.add(tf.layers.dense({units: 10, activation: 'relu'}));
model.add(tf.layers.dense({units: 2, activation: 'softmax'}));

// print out model summary
//model.summary();

// compile
document.getElementById("action" ).innerHTML = 'Compilation';
model.compile({loss: "categoricalCrossentropy", optimizer: "rmsprop",metrics: ['accuracy']});
// fit
async function train() {
await model.fit(x_train, y_train,{ batchSize: 2,epochs: epochs ,shuffle: true,
    callbacks: tfvis.show.fitCallbacks(
      { name: 'Training Performance' },
      ['loss', 'acc'], 
      { height: 200, callbacks: ['onEpochEnd'] }
    )});
}
	
document.getElementById("action" ).innerHTML = 'Training';

train();
	
document.getElementById("action" ).innerHTML = 'Prediction';

var predictionsTensor =model.predict(x_test, {batchSize: 5});
//predictions = Array.from(predictions.dataSync());
predictions = predictionsTensor.arraySync()

//numerisation des predictions
y_predictions=[]
for (let pred of predictions){
	let vector=[]
	while ( vector.length<nb_class){vector.push(0)}
	for (let i in pred){
		if (pred[i]>0.5){vector[i]=1;}
		}
	y_predictions.push(vector)
}

var True=0;
for (let i in y_test){
	if(y_predictions[i].indexOf(1)==y_test[i].indexOf(1)){True++;}
	}
var accuracy=True/y_test.length;
	
document.getElementById("action" ).innerHTML = 'Accuracy  : '+accuracy;

document.getElementById("action" ).innerHTML = 'End of the script.';
}
</script>
</html>
